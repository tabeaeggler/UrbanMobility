{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "pd.options.display.max_seq_items = 2000\n",
    "\n",
    "# import python modules\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('/Users/tabea/Documents/UrbanMobility/src')\n",
    "from data_prep import journey_data_aggregation as agg"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Aggregation and Analysis"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tabea/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "# import files\n",
    "journey_16 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2016.csv', parse_dates=['end_date', 'start_date'], index_col=0)\n",
    "journey_17 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2017.csv', parse_dates=['end_date', 'start_date'], index_col=0)\n",
    "journey_18 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2018.csv', parse_dates=['end_date', 'start_date'], index_col=0)\n",
    "journey_19 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2019.csv', parse_dates=['end_date', 'start_date'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hourly aggregation    \n",
    "agg_journey_16 = agg.aggregate_demand(journey_16, 'H')\n",
    "agg_journey_17 = agg.aggregate_demand(journey_17, 'H')\n",
    "agg_journey_18 = agg.aggregate_demand(journey_18, 'H')\n",
    "agg_journey_19 = agg.aggregate_demand(journey_19, 'H')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adding rows for hours where no journeys were started, which are missing from the data. This makes sure that every hour is represented in the data, even those with a demand of 0.\n",
    "borough_df = pd.read_csv('../data/interim/borough_data_featureeng.csv', index_col=0)\n",
    "temporal_weather_features = ['hour', 'part_of_day', 'day_of_week', 'day_of_month', 'day_of_year', 'is_weekend', 'month', 'season', 'bank_holiday', 'temp', 'feelslike', 'dew', 'humidity', 'precip', 'windgust', 'windspeed', 'cloudcover', 'visibility', 'uvindex']\n",
    "\n",
    "agg_journey_16_added_0_demand = agg.clean_aggregated_df_hourly(agg_journey_16, borough_df, temporal_weather_features)\n",
    "agg_journey_17_added_0_demand = agg.clean_aggregated_df_hourly(agg_journey_17, borough_df, temporal_weather_features)\n",
    "agg_journey_18_added_0_demand = agg.clean_aggregated_df_hourly(agg_journey_18, borough_df, temporal_weather_features)\n",
    "agg_journey_19_added_0_demand = agg.clean_aggregated_df_hourly(agg_journey_19, borough_df, temporal_weather_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2016:  0\n",
      "2017:  0\n",
      "2017:  0\n",
      "2019:  0\n"
     ]
    }
   ],
   "source": [
    "# print nr of nan values\n",
    "print('2016: ', agg_journey_16_added_0_demand.isna().sum().sum())\n",
    "print('2017: ', agg_journey_17_added_0_demand.isna().sum().sum())\n",
    "print('2017: ', agg_journey_18_added_0_demand.isna().sum().sum())\n",
    "print('2019: ', agg_journey_19_added_0_demand.isna().sum().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save aggregated data\n",
    "agg_journey_16_added_0_demand.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_hourly_2016.csv')\n",
    "agg_journey_17_added_0_demand.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_hourly_2017.csv')\n",
    "agg_journey_18_added_0_demand.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_hourly_2018.csv')\n",
    "agg_journey_19_added_0_demand.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_hourly_2019.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily Data Aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/tabea/opt/anaconda3/lib/python3.9/site-packages/numpy/lib/arraysetops.py:583: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  mask |= (ar1 == a)\n"
     ]
    }
   ],
   "source": [
    "journey_16 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2016_dailyweather.csv', parse_dates=['end_date', 'start_date'], index_col=0)\n",
    "journey_17 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2017_dailyweather.csv', parse_dates=['end_date', 'start_date'], index_col=0)\n",
    "journey_18 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2018_dailyweather.csv', parse_dates=['end_date', 'start_date'], index_col=0)\n",
    "journey_19 = pd.read_csv('../data/processed/journey_data_clean_featureeng/journey_data_2019_dailyweather.csv', parse_dates=['end_date', 'start_date'], index_col=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily aggregation    \n",
    "agg_journey_daily_16 = agg.aggregate_demand(journey_16, 'D')\n",
    "agg_journey_daily_17 = agg.aggregate_demand(journey_17, 'D')\n",
    "agg_journey_daily_18 = agg.aggregate_demand(journey_18, 'D')\n",
    "agg_journey_daily_19 = agg.aggregate_demand(journey_19, 'D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean\n",
    "agg_journey_daily_16_cleaned = agg.clean_aggregated_df_daily(agg_journey_daily_16)\n",
    "agg_journey_daily_17_cleaned = agg.clean_aggregated_df_daily(agg_journey_daily_17)\n",
    "agg_journey_daily_18_cleaned = agg.clean_aggregated_df_daily(agg_journey_daily_18)\n",
    "agg_journey_daily_19_cleaned = agg.clean_aggregated_df_daily(agg_journey_daily_19)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg_journey_daily_16_cleaned.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_daily_2016.csv')\n",
    "agg_journey_daily_17_cleaned.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_daily_2017.csv')\n",
    "agg_journey_daily_18_cleaned.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_daily_2018.csv')\n",
    "agg_journey_daily_19_cleaned.to_csv('../data/processed/aggregated_journey_data/agg_journey_data_daily_2019.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hourly Aggregated Demand Analysis: Detailed Weekly Demand 2019 and Yearly Comparisons "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "boroughs = ['Westminster', 'Tower Hamlets', 'Kensington and Chelsea', 'Camden', 'Hammersmith and Fulham', 'Lambeth', 'Wandsworth', 'Southwark', \n",
    "            'Hackney', 'City of London', 'Islington', 'Newham']\n",
    "\n",
    "borough_data = {}\n",
    "\n",
    "for borough in boroughs:\n",
    "    borough_data[f'weekly_demand_{borough}_2019'], borough_data[f'weekly_demand_{borough}_2018'], borough_data[f'weekly_demand_{borough}_2017'], borough_data[f'weekly_demand_{borough}_2016'] = agg.plot_demand_by_week_borough(agg_journey_19_added_0_demand, agg_journey_18_added_0_demand, agg_journey_17_added_0_demand, agg_journey_16_added_0_demand, borough)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "0 - 23 hours corresponds to Monday. The data from 01.01.2017, which is a Sunday, is shown towards the end of the weekly cycle in the visualization, at hours 144-167. This might seem counter-intuitive if you are used to calendars where Sunday is the first day of the week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>demand</th>\n",
       "      <th>start_date_hour</th>\n",
       "      <th>hour</th>\n",
       "      <th>hour_of_week</th>\n",
       "      <th>week_of_year</th>\n",
       "      <th>day_of_week</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>44525</th>\n",
       "      <td>29</td>\n",
       "      <td>2018-06-10 00:00:00</td>\n",
       "      <td>0</td>\n",
       "      <td>144</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44537</th>\n",
       "      <td>18</td>\n",
       "      <td>2018-06-10 01:00:00</td>\n",
       "      <td>1</td>\n",
       "      <td>145</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44549</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-10 02:00:00</td>\n",
       "      <td>2</td>\n",
       "      <td>146</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44560</th>\n",
       "      <td>11</td>\n",
       "      <td>2018-06-10 03:00:00</td>\n",
       "      <td>3</td>\n",
       "      <td>147</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44571</th>\n",
       "      <td>4</td>\n",
       "      <td>2018-06-10 04:00:00</td>\n",
       "      <td>4</td>\n",
       "      <td>148</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44583</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-10 05:00:00</td>\n",
       "      <td>5</td>\n",
       "      <td>149</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44594</th>\n",
       "      <td>2</td>\n",
       "      <td>2018-06-10 06:00:00</td>\n",
       "      <td>6</td>\n",
       "      <td>150</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44605</th>\n",
       "      <td>13</td>\n",
       "      <td>2018-06-10 07:00:00</td>\n",
       "      <td>7</td>\n",
       "      <td>151</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44617</th>\n",
       "      <td>13</td>\n",
       "      <td>2018-06-10 08:00:00</td>\n",
       "      <td>8</td>\n",
       "      <td>152</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44629</th>\n",
       "      <td>25</td>\n",
       "      <td>2018-06-10 09:00:00</td>\n",
       "      <td>9</td>\n",
       "      <td>153</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44641</th>\n",
       "      <td>24</td>\n",
       "      <td>2018-06-10 10:00:00</td>\n",
       "      <td>10</td>\n",
       "      <td>154</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44653</th>\n",
       "      <td>16</td>\n",
       "      <td>2018-06-10 11:00:00</td>\n",
       "      <td>11</td>\n",
       "      <td>155</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44665</th>\n",
       "      <td>15</td>\n",
       "      <td>2018-06-10 12:00:00</td>\n",
       "      <td>12</td>\n",
       "      <td>156</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44677</th>\n",
       "      <td>9</td>\n",
       "      <td>2018-06-10 13:00:00</td>\n",
       "      <td>13</td>\n",
       "      <td>157</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44689</th>\n",
       "      <td>12</td>\n",
       "      <td>2018-06-10 14:00:00</td>\n",
       "      <td>14</td>\n",
       "      <td>158</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44701</th>\n",
       "      <td>13</td>\n",
       "      <td>2018-06-10 15:00:00</td>\n",
       "      <td>15</td>\n",
       "      <td>159</td>\n",
       "      <td>23</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       demand     start_date_hour  hour  hour_of_week  week_of_year  \\\n",
       "44525      29 2018-06-10 00:00:00     0           144            23   \n",
       "44537      18 2018-06-10 01:00:00     1           145            23   \n",
       "44549       2 2018-06-10 02:00:00     2           146            23   \n",
       "44560      11 2018-06-10 03:00:00     3           147            23   \n",
       "44571       4 2018-06-10 04:00:00     4           148            23   \n",
       "44583       2 2018-06-10 05:00:00     5           149            23   \n",
       "44594       2 2018-06-10 06:00:00     6           150            23   \n",
       "44605      13 2018-06-10 07:00:00     7           151            23   \n",
       "44617      13 2018-06-10 08:00:00     8           152            23   \n",
       "44629      25 2018-06-10 09:00:00     9           153            23   \n",
       "44641      24 2018-06-10 10:00:00    10           154            23   \n",
       "44653      16 2018-06-10 11:00:00    11           155            23   \n",
       "44665      15 2018-06-10 12:00:00    12           156            23   \n",
       "44677       9 2018-06-10 13:00:00    13           157            23   \n",
       "44689      12 2018-06-10 14:00:00    14           158            23   \n",
       "44701      13 2018-06-10 15:00:00    15           159            23   \n",
       "\n",
       "       day_of_week  \n",
       "44525            6  \n",
       "44537            6  \n",
       "44549            6  \n",
       "44560            6  \n",
       "44571            6  \n",
       "44583            6  \n",
       "44594            6  \n",
       "44605            6  \n",
       "44617            6  \n",
       "44629            6  \n",
       "44641            6  \n",
       "44653            6  \n",
       "44665            6  \n",
       "44677            6  \n",
       "44689            6  \n",
       "44701            6  "
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# inspect specific days\n",
    "df_inspect = borough_data['weekly_demand_City of London_2018']\n",
    "\n",
    "df_inspect = df_inspect[(df_inspect['week_of_year'] == 23) & (df_inspect['hour_of_week'].isin(range(144, 160)))]\n",
    "df_inspect[['demand', 'start_date_hour', 'hour', 'hour_of_week', 'week_of_year', 'day_of_week']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
