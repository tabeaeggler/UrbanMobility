{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9dcdf0d3",
   "metadata": {
    "id": "9dcdf0d3"
   },
   "source": [
    "# Get and clean Journey, Bike location and Borough Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "de9dc53e",
   "metadata": {
    "id": "de9dc53e"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "import io\n",
    "import json\n",
    "import urllib\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import datetime\n",
    "from rapidfuzz import fuzz\n",
    "from concurrent import futures"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbfcfc9d",
   "metadata": {
    "id": "fbfcfc9d"
   },
   "source": [
    "## 1. IMPORT BIKE JOURNEY DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35c41fc5",
   "metadata": {},
   "source": [
    "### fetch data\n",
    "Due to the dynamic loading of the data, web scraping is not possible. Therefore, the name of the files are copy & pased into a CSV file which then get fetched and combined with multiple API requests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "822bbb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_columns(df):\n",
    "    \"\"\"\n",
    "    This function renames the columns in the provided dataframe 'df' as per the \n",
    "    mapping defined in 'column_names', and also changes the datatype of some columns.\n",
    "    \"\"\"\n",
    "    \n",
    "    # define a mapping of old column names to new standardized names\n",
    "    column_names = {\n",
    "        'End Station Id': 'EndStation Id',\n",
    "        'End station number': 'EndStation Id',\n",
    "        'Start Station Id': 'StartStation Id',\n",
    "        'Start station number': 'StartStation Id',\n",
    "        'End Station Name': 'EndStation Name',\n",
    "        'End station': 'EndStation Name',\n",
    "        'Start Station Name': 'StartStation Name',\n",
    "        'Start station': 'StartStation Name',\n",
    "        'Start date': 'Start Date',\n",
    "        'End Date': 'End Date',\n",
    "        'End date': 'End Date',\n",
    "        'Number': 'Rental Id',\n",
    "    }\n",
    "    \n",
    "    for old_name, new_name in column_names.items():\n",
    "            if old_name in df.columns:\n",
    "                df = df.rename(columns={old_name: new_name})\n",
    "                if new_name in ['EndStation Id', 'StartStation Id', 'Rental Id']:\n",
    "                    df[new_name] = pd.to_numeric(df[new_name], errors='coerce', downcast='integer')\n",
    "                elif new_name in ['Start Date', 'End Date']:\n",
    "                    df[new_name] = pd.to_datetime(df[new_name], infer_datetime_format=True)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3da34c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the list of file names from a CSV file\n",
    "filenames = pd.read_csv('/Users/tabea/Documents/UrbanMobility/filenames-data.csv', header=None, squeeze=True)\n",
    "\n",
    "# combine a list of URL by add the base-url and filename\n",
    "base_url = 'http://cycling.data.tfl.gov.uk/usage-stats/'\n",
    "url_list = (base_url + urllib.parse.quote(x) for x in filenames)\n",
    "unused_cols = ['Total duration (ms)', 'Total duration', 'Duration', 'Duration_Seconds', 'Bike Id', 'Bike number', 'Bike model']\n",
    "\n",
    "# loop through each URL to extract data\n",
    "temp_dfs = []\n",
    "for url in url_list:\n",
    "    response = requests.get(url, verify=False, timeout=(3, 7))\n",
    "\n",
    "    if url.endswith('.csv'):\n",
    "        temp_df = pd.read_csv(io.StringIO(response.content.decode('utf-8')), usecols=lambda col: col not in unused_cols)\n",
    "\n",
    "    elif url.endswith('.xlsx'):\n",
    "        temp_df = pd.read_excel(io.BytesIO(response.content), usecols=lambda col: col not in unused_cols)\n",
    "\n",
    "    temp_df = rename_columns(temp_df)\n",
    "    temp_dfs.append(temp_df)\n",
    "\n",
    "# concatenate all temporary dataframes into a single dataframe\n",
    "merged_df = pd.concat(temp_dfs, ignore_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "292tJkGISg90",
   "metadata": {
    "id": "292tJkGISg90"
   },
   "outputs": [],
   "source": [
    "merged_df.to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "71b301a1",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 181
    },
    "id": "71b301a1",
    "outputId": "14181a83-439e-4f78-a827-80c7613b6720"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84188068"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# total amount of entries: 84'188'068\n",
    "len(merged_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6622777c",
   "metadata": {},
   "source": [
    "### import data from disk (if already fetched)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "052a3439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import\n",
    "merged_df = pd.read_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_raw.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "287d0920",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Unnamed: 0.1</th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Id</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Id</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>Unnamed: 9</th>\n",
       "      <th>Unnamed: 10</th>\n",
       "      <th>Unnamed: 11</th>\n",
       "      <th>EndStation Logical Terminal</th>\n",
       "      <th>endStationPriority_id</th>\n",
       "      <th>StartStation Logical Terminal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>63097899.0</td>\n",
       "      <td>2017-03-15 00:06:00</td>\n",
       "      <td>631.0</td>\n",
       "      <td>Battersea Park Road, Nine Elms</td>\n",
       "      <td>2017-03-15 00:00:00</td>\n",
       "      <td>74.0</td>\n",
       "      <td>Vauxhall Cross, Vauxhall</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>63097900.0</td>\n",
       "      <td>2017-03-15 00:05:00</td>\n",
       "      <td>397.0</td>\n",
       "      <td>Devonshire Terrace, Bayswater</td>\n",
       "      <td>2017-03-15 00:01:00</td>\n",
       "      <td>410.0</td>\n",
       "      <td>Edgware Road Station, Marylebone</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>63097901.0</td>\n",
       "      <td>2017-03-15 00:06:00</td>\n",
       "      <td>426.0</td>\n",
       "      <td>Vincent Street, Pimlico</td>\n",
       "      <td>2017-03-15 00:01:00</td>\n",
       "      <td>177.0</td>\n",
       "      <td>Ashley Place, Victoria</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>63097902.0</td>\n",
       "      <td>2017-03-15 00:12:00</td>\n",
       "      <td>462.0</td>\n",
       "      <td>Bonny Street, Camden Town</td>\n",
       "      <td>2017-03-15 00:01:00</td>\n",
       "      <td>22.0</td>\n",
       "      <td>Northington Street , Holborn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>63097903.0</td>\n",
       "      <td>2017-03-15 00:05:00</td>\n",
       "      <td>423.0</td>\n",
       "      <td>Eaton Square (South), Belgravia</td>\n",
       "      <td>2017-03-15 00:01:00</td>\n",
       "      <td>143.0</td>\n",
       "      <td>Pont Street, Knightsbridge</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Unnamed: 0.1   Rental Id             End Date  EndStation Id  \\\n",
       "0           0             0  63097899.0  2017-03-15 00:06:00          631.0   \n",
       "1           1             1  63097900.0  2017-03-15 00:05:00          397.0   \n",
       "2           2             2  63097901.0  2017-03-15 00:06:00          426.0   \n",
       "3           3             3  63097902.0  2017-03-15 00:12:00          462.0   \n",
       "4           4             4  63097903.0  2017-03-15 00:05:00          423.0   \n",
       "\n",
       "                   EndStation Name           Start Date  StartStation Id  \\\n",
       "0   Battersea Park Road, Nine Elms  2017-03-15 00:00:00             74.0   \n",
       "1    Devonshire Terrace, Bayswater  2017-03-15 00:01:00            410.0   \n",
       "2          Vincent Street, Pimlico  2017-03-15 00:01:00            177.0   \n",
       "3        Bonny Street, Camden Town  2017-03-15 00:01:00             22.0   \n",
       "4  Eaton Square (South), Belgravia  2017-03-15 00:01:00            143.0   \n",
       "\n",
       "                  StartStation Name  Unnamed: 9  Unnamed: 10  Unnamed: 11  \\\n",
       "0          Vauxhall Cross, Vauxhall         NaN          NaN          NaN   \n",
       "1  Edgware Road Station, Marylebone         NaN          NaN          NaN   \n",
       "2            Ashley Place, Victoria         NaN          NaN          NaN   \n",
       "3      Northington Street , Holborn         NaN          NaN          NaN   \n",
       "4        Pont Street, Knightsbridge         NaN          NaN          NaN   \n",
       "\n",
       "   EndStation Logical Terminal  endStationPriority_id  \\\n",
       "0                          NaN                    NaN   \n",
       "1                          NaN                    NaN   \n",
       "2                          NaN                    NaN   \n",
       "3                          NaN                    NaN   \n",
       "4                          NaN                    NaN   \n",
       "\n",
       "   StartStation Logical Terminal  \n",
       "0                            NaN  \n",
       "1                            NaN  \n",
       "2                            NaN  \n",
       "3                            NaN  \n",
       "4                            NaN  "
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36945983",
   "metadata": {
    "id": "36945983"
   },
   "source": [
    "## 2. CLEAN BIKE JOURNEY DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "V4KsfJjNPnQ0",
   "metadata": {
    "id": "V4KsfJjNPnQ0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length before cleaning: 84188068\n"
     ]
    }
   ],
   "source": [
    "print(\"length before cleaning:\", len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da0d1764",
   "metadata": {},
   "source": [
    "### drop columns starting with 'Unnamed'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "4f838a5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.filter(regex='^(?!Unnamed)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef158ecc",
   "metadata": {},
   "source": [
    "### drop rows with nan values only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "93a91139",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = merged_df.dropna(how='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a448dc25",
   "metadata": {},
   "source": [
    "### investigate duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "a12007cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "duplicated rental ID samples count:  585398\n"
     ]
    }
   ],
   "source": [
    "# some files have same or overlapping content, but different names. \n",
    "# e.g: 01b Journey Data Extract 24Jan16-06Feb16.csv, 01bJourneyDataExtract24Jan16-06Feb16.csv\n",
    "    \n",
    "duplicates_rental_id = merged_df[merged_df['Rental Id'].duplicated(keep=False)]\n",
    "print(\"duplicated rental ID samples count: \", len(duplicates_rental_id))\n",
    "# duplicates_rental_id.to_csv('/Users/tabea/Documents/UrbanMobility/data/duplicates.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a252562",
   "metadata": {},
   "source": [
    "### drop duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "x2tK4QaAOqcw",
   "metadata": {
    "id": "x2tK4QaAOqcw"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current length of df:  83895356\n"
     ]
    }
   ],
   "source": [
    "# drop all samples with duplicated rental id, sort first to keep the row with the most non-null values\n",
    "\n",
    "merged_df['nonnull_count'] = merged_df.notnull().sum(axis=1)\n",
    "merged_df = merged_df.sort_values(by=['Rental Id', 'nonnull_count'], ascending=[True, False])\n",
    "merged_df = merged_df.drop_duplicates(subset='Rental Id', keep='first')\n",
    "merged_df = merged_df.drop(columns='nonnull_count')\n",
    "\n",
    "print(\"current length of df: \", len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb59cbec",
   "metadata": {},
   "source": [
    "### investigate all nan values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "7812b6cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rental Id                               0\n",
      "End Date                           170358\n",
      "EndStation Id                      715522\n",
      "EndStation Name                    171824\n",
      "Start Date                              0\n",
      "StartStation Id                    234440\n",
      "StartStation Name                       0\n",
      "EndStation Logical Terminal      83665717\n",
      "endStationPriority_id            83665717\n",
      "StartStation Logical Terminal    83662856\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(merged_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc28dfb3",
   "metadata": {},
   "source": [
    "### nan values: StartStation Name & EndStation Name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "d46dc7ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartStation Name NaNs count:  0\n",
      "EndStation Name NaNs: count   171824\n"
     ]
    }
   ],
   "source": [
    "StartStationName_NAN = merged_df[merged_df[\"StartStation Name\"].isna()]\n",
    "print(\"StartStation Name NaNs count: \", len(StartStationName_NAN))\n",
    "\n",
    "EndStationName_NAN = merged_df[merged_df[\"EndStation Name\"].isna()]\n",
    "print(\"EndStation Name NaNs: count  \", len(EndStationName_NAN))\n",
    "# EndStationName_NAN.to_csv('/Users/tabea/Documents/UrbanMobility/data/EndStationName_NAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "c931b629",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current df length: 83723532\n"
     ]
    }
   ],
   "source": [
    "# EndStation Name is only NaN if EndStation ID is also NaN -> they can't be mapped, so they must be removed.\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['EndStation Id', 'EndStation Name'], how='all')\n",
    "print(\"current df length:\", len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72983817",
   "metadata": {},
   "source": [
    "### nan values: Start Date and End Date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "bdb8ca9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start Date NaNs:  0\n",
      "End Date NaNs:  69\n"
     ]
    }
   ],
   "source": [
    "StartDate_NAN = merged_df[merged_df[\"Start Date\"].isna()]\n",
    "print(\"Start Date NaNs: \", len(StartDate_NAN))\n",
    "\n",
    "EndDate_NAN = merged_df[merged_df[\"End Date\"].isna()]\n",
    "print(\"End Date NaNs: \", len(EndDate_NAN))\n",
    "# EndDate_NAN.to_csv('/Users/tabea/Documents/UrbanMobility/data/EndDate_NAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "61495287",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current df length: 83723463\n"
     ]
    }
   ],
   "source": [
    "# drop 69 entries with missing data\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['End Date'])\n",
    "print(\"current df length:\", len(merged_df))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "287e0725",
   "metadata": {},
   "source": [
    "### nan values: StartStation Id & EndStation Id\n",
    "Numerous NaN values are observed in the 'StartStation Id' and 'EndStation Id' columns. The primary cause: cycling rides extending beyond a single calendar day. For these instances, stations are referred to as 'TerminalStation', each carrying a unique ID set with higher numbers (>852).\n",
    "\n",
    "Due to the mix of stationID and terminalID and lots of NaN values, the ID's get dropped and the name of the station is used as identifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "38cd7dba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartStation Id NaNs count:  231579\n",
      "EndStation Id NaNs count:  543698\n"
     ]
    }
   ],
   "source": [
    "# StartStation Id: 234'440 NaN -> but most StartStation Names are known\n",
    "StartStationId_NAN = merged_df[merged_df[\"StartStation Id\"].isna()]\n",
    "print(\"StartStation Id NaNs count: \", len(StartStationId_NAN))\n",
    "# StartStationId_NAN.to_csv('/Users/tabea/Documents/UrbanMobility/data/StartStationId_NAN.csv')\n",
    "\n",
    "# EndStation Id: 715'522 NaN -> but most EndStation Names are known\n",
    "EndStationId_NAN = merged_df[merged_df[\"EndStation Id\"].isna()]\n",
    "print(\"EndStation Id NaNs count: \", len(EndStationId_NAN))\n",
    "# EndStationId_NAN.to_csv('/Users/tabea/Documents/UrbanMobility/data/EndStationId_NAN.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "ead5d81a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count of terminal station ID instaed of normal ID:  2788522\n"
     ]
    }
   ],
   "source": [
    "# only 852 station are present in the data. But there are also terminal station IDs that have higher values and are mixed in the data.\n",
    "# they can be found here: https://api.tfl.gov.uk/BikePoint/\n",
    "\n",
    "greater_than_852 = (merged_df['StartStation Id'] > 852) | (merged_df['EndStation Id'] > 852)\n",
    "print(\"count of terminal station ID instaed of normal ID: \", greater_than_852.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "5eea5529",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rental Id            0\n",
      "End Date             0\n",
      "EndStation Name      0\n",
      "Start Date           0\n",
      "StartStation Name    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop ID's\n",
    "\n",
    "merged_df = merged_df.drop(columns=['StartStation Id', 'EndStation Id', 'EndStation Logical Terminal', 'endStationPriority_id', 'StartStation Logical Terminal'])\n",
    "print(merged_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88156010",
   "metadata": {},
   "source": [
    "### change dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5359c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df[\"Rental Id\"] = merged_df[\"Rental Id\"].astype(int)\n",
    "merged_df[\"Start Date\"] = pd.to_datetime(merged_df[\"Start Date\"])\n",
    "merged_df[\"End Date\"] = pd.to_datetime(merged_df[\"End Date\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25c94ed5",
   "metadata": {},
   "source": [
    "### first cleaning: done\n",
    "464'605 samples got deleted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "9JSbvAxpPuIA",
   "metadata": {
    "id": "9JSbvAxpPuIA"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length df after cleaning: 83723463\n"
     ]
    }
   ],
   "source": [
    "print(\"length df after cleaning:\", len(merged_df))\n",
    "# merged_df.to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_cleaned.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d28abc8",
   "metadata": {
    "id": "dbf93785"
   },
   "source": [
    "# 4. MAP BIKE STATION LOCATIONS AND BOROUGHS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eaea55d",
   "metadata": {},
   "source": [
    "### get all bike stations and its corresponding locations data (lat and lon)\n",
    "by extracting relevant information from API requests to tfl BikePoint data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "12d281d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(796, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>terminalId</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>001023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Phillimore Gardens, Kensington</td>\n",
       "      <td>51.499606</td>\n",
       "      <td>-0.197574</td>\n",
       "      <td>001018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Christopher Street, Liverpool Street</td>\n",
       "      <td>51.521283</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>001012</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>St. Chad's Street, King's Cross</td>\n",
       "      <td>51.530059</td>\n",
       "      <td>-0.120973</td>\n",
       "      <td>001013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Sedding Street, Sloane Square</td>\n",
       "      <td>51.493130</td>\n",
       "      <td>-0.156876</td>\n",
       "      <td>003420</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  id                                  name        lat       lon terminalId\n",
       "0  1            River Street , Clerkenwell  51.529163 -0.109970     001023\n",
       "1  2        Phillimore Gardens, Kensington  51.499606 -0.197574     001018\n",
       "2  3  Christopher Street, Liverpool Street  51.521283 -0.084605     001012\n",
       "3  4       St. Chad's Street, King's Cross  51.530059 -0.120973     001013\n",
       "4  5         Sedding Street, Sloane Square  51.493130 -0.156876     003420"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://api.tfl.gov.uk/BikePoint/\"\n",
    "response = requests.get(url)\n",
    "root = json.loads(response.text)\n",
    "\n",
    "data = []\n",
    "logical_id = \"\"\n",
    "for station in root:\n",
    "    for prop in station['additionalProperties']:\n",
    "        if prop['key'] == 'TerminalName':\n",
    "            logical_id = prop['value']\n",
    "            break\n",
    "\n",
    "    station_data = {\n",
    "        \"id\": station['id'][11:],\n",
    "        \"name\": station['commonName'],\n",
    "        \"lat\": station['lat'],\n",
    "        \"lon\": station['lon'],\n",
    "        \"terminalId\": logical_id\n",
    "    }\n",
    "    data.append(station_data)\n",
    "\n",
    "bike_locs = pd.DataFrame(data)\n",
    "\n",
    "print(bike_locs.shape)\n",
    "bike_locs.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9b0fec",
   "metadata": {},
   "source": [
    "### get boroughs based on location and map to bike location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "0ac86f60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_borough(lat, lon):\n",
    "    \"\"\"\n",
    "    Function to retrieve borough name using lat and lon coordinates.\n",
    "    \n",
    "    This function sends a GET request to the 'findthatpostcode' API, using \n",
    "    the provided lat and lon coordinates. If the request is successful, the \n",
    "    function extracts the borough name from the response data and returns it. \n",
    "    If the request is unsuccessful, the function returns 'no borough'.\n",
    "    \n",
    "    Parameters:\n",
    "    lat (float): Latitude coordinate of the location.\n",
    "    lon (float): Longitude coordinate of the location.\n",
    "\n",
    "    Returns:\n",
    "    str: Borough name or 'no borough' if the API request is unsuccessful.\n",
    "    \"\"\"\n",
    "    url = f'https://findthatpostcode.uk/points/{lat},{lon}'\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        data = response.json()\n",
    "        borough = data['included'][0]['attributes']['cty_name']\n",
    "        return borough\n",
    "    else:\n",
    "        return 'no borough'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "95400ec8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>name</th>\n",
       "      <th>lat</th>\n",
       "      <th>lon</th>\n",
       "      <th>terminalId</th>\n",
       "      <th>borough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>River Street , Clerkenwell</td>\n",
       "      <td>51.529163</td>\n",
       "      <td>-0.109970</td>\n",
       "      <td>1023</td>\n",
       "      <td>Islington</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Phillimore Gardens, Kensington</td>\n",
       "      <td>51.499606</td>\n",
       "      <td>-0.197574</td>\n",
       "      <td>1018</td>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Christopher Street, Liverpool Street</td>\n",
       "      <td>51.521283</td>\n",
       "      <td>-0.084605</td>\n",
       "      <td>1012</td>\n",
       "      <td>Hackney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>St. Chad's Street, King's Cross</td>\n",
       "      <td>51.530059</td>\n",
       "      <td>-0.120973</td>\n",
       "      <td>1013</td>\n",
       "      <td>Camden</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Sedding Street, Sloane Square</td>\n",
       "      <td>51.493130</td>\n",
       "      <td>-0.156876</td>\n",
       "      <td>3420</td>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                  name        lat       lon  terminalId  \\\n",
       "0   1            River Street , Clerkenwell  51.529163 -0.109970        1023   \n",
       "1   2        Phillimore Gardens, Kensington  51.499606 -0.197574        1018   \n",
       "2   3  Christopher Street, Liverpool Street  51.521283 -0.084605        1012   \n",
       "3   4       St. Chad's Street, King's Cross  51.530059 -0.120973        1013   \n",
       "4   5         Sedding Street, Sloane Square  51.493130 -0.156876        3420   \n",
       "\n",
       "                  borough  \n",
       "0               Islington  \n",
       "1  Kensington and Chelsea  \n",
       "2                 Hackney  \n",
       "3                  Camden  \n",
       "4  Kensington and Chelsea  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# map borough to the bike locations\n",
    "bike_locs['borough'] = bike_locs.apply(lambda row: get_borough(row['lat'], row['lon']), axis=1)\n",
    "bike_locs.head(5)\n",
    "# bike_locs.to_csv('/Users/tabea/Documents/UrbanMobility/data/bike_locations_boroughs.csv', header=True, index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "894e99c2",
   "metadata": {},
   "source": [
    "## 5. ADD BOROUGH DATA TO JOURNEY DATA"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94de9aa6",
   "metadata": {},
   "source": [
    "### Map the StartStation and EndStation Names to boroughs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "5d431591",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Standardize by stripping white space and converting to lower case, create dictionary for mapping\n",
    "bike_locs['name'] = bike_locs['name'].str.strip().str.lower()\n",
    "borough_mapping = bike_locs.set_index('name')['borough'].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1a78794",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Rental Id</th>\n",
       "      <th>End Date</th>\n",
       "      <th>EndStation Name</th>\n",
       "      <th>Start Date</th>\n",
       "      <th>StartStation Name</th>\n",
       "      <th>StartBorough</th>\n",
       "      <th>EndBorough</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29879046</td>\n",
       "      <td>40346508</td>\n",
       "      <td>2015-04-01 00:06:00</td>\n",
       "      <td>Ebury Bridge, Pimlico</td>\n",
       "      <td>2015-04-01 00:00:00</td>\n",
       "      <td>Harriet Street, Knightsbridge</td>\n",
       "      <td>Kensington and Chelsea</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>29879054</td>\n",
       "      <td>40346509</td>\n",
       "      <td>2015-04-01 00:11:00</td>\n",
       "      <td>Regent's Row , Haggerston</td>\n",
       "      <td>2015-04-01 00:00:00</td>\n",
       "      <td>Brushfield Street, Liverpool Street</td>\n",
       "      <td>City of London</td>\n",
       "      <td>Hackney</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>29879048</td>\n",
       "      <td>40346510</td>\n",
       "      <td>2015-04-01 00:08:00</td>\n",
       "      <td>Foley Street, Fitzrovia</td>\n",
       "      <td>2015-04-01 00:01:00</td>\n",
       "      <td>Tavistock Place, Bloomsbury</td>\n",
       "      <td>Camden</td>\n",
       "      <td>Westminster</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>29879161</td>\n",
       "      <td>40346511</td>\n",
       "      <td>2015-04-01 00:50:00</td>\n",
       "      <td>Bow Church Station, Bow</td>\n",
       "      <td>2015-04-01 00:01:00</td>\n",
       "      <td>Moor Street, Soho</td>\n",
       "      <td>Westminster</td>\n",
       "      <td>Tower Hamlets</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>29879044</td>\n",
       "      <td>40346512</td>\n",
       "      <td>2015-04-01 00:03:00</td>\n",
       "      <td>Jubilee Street, Stepney</td>\n",
       "      <td>2015-04-01 00:01:00</td>\n",
       "      <td>Philpot Street, Whitechapel</td>\n",
       "      <td>Tower Hamlets</td>\n",
       "      <td>Tower Hamlets</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Rental Id             End Date             EndStation Name  \\\n",
       "0    29879046   40346508  2015-04-01 00:06:00       Ebury Bridge, Pimlico   \n",
       "1    29879054   40346509  2015-04-01 00:11:00  Regent's Row , Haggerston    \n",
       "2    29879048   40346510  2015-04-01 00:08:00     Foley Street, Fitzrovia   \n",
       "3    29879161   40346511  2015-04-01 00:50:00     Bow Church Station, Bow   \n",
       "4    29879044   40346512  2015-04-01 00:03:00     Jubilee Street, Stepney   \n",
       "\n",
       "            Start Date                    StartStation Name  \\\n",
       "0  2015-04-01 00:00:00        Harriet Street, Knightsbridge   \n",
       "1  2015-04-01 00:00:00  Brushfield Street, Liverpool Street   \n",
       "2  2015-04-01 00:01:00          Tavistock Place, Bloomsbury   \n",
       "3  2015-04-01 00:01:00                    Moor Street, Soho   \n",
       "4  2015-04-01 00:01:00          Philpot Street, Whitechapel   \n",
       "\n",
       "             StartBorough     EndBorough  \n",
       "0  Kensington and Chelsea    Westminster  \n",
       "1          City of London        Hackney  \n",
       "2                  Camden    Westminster  \n",
       "3             Westminster  Tower Hamlets  \n",
       "4           Tower Hamlets  Tower Hamlets  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# mapping\n",
    "merged_df['StartBorough'] = merged_df['StartStation Name'].str.strip().str.lower().map(borough_mapping)\n",
    "merged_df['EndBorough'] = merged_df['EndStation Name'].str.strip().str.lower().map(borough_mapping)\n",
    "\n",
    "merged_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1016c0c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                 0\n",
      "Rental Id                  0\n",
      "End Date                   0\n",
      "EndStation Name            0\n",
      "Start Date                 0\n",
      "StartStation Name          0\n",
      "StartBorough         3104758\n",
      "EndBorough           3192763\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# still lots of missing boroughs: 3'104'758 + 3'192'763\n",
    "print(merged_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c7970f5",
   "metadata": {},
   "source": [
    "### Run fuzzy matching for empty boroughs\n",
    "Matching names that refer to the same station but are slightly different in their naming. Parallel processing to improve the performance of fuzzy matching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7a734b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fuzzy_match(station_name, min_score=70):\n",
    "    \"\"\"\n",
    "    Performs fuzzy matching between a given station name and a mapping of station names to boroughs.\n",
    "    \n",
    "    Args:\n",
    "        station_name (str): The station name to be matched.\n",
    "        min_score (int): The minimum similarity score required for a match (default: 70).\n",
    "    \n",
    "    Returns:\n",
    "        str or None: The borough corresponding to the best fuzzy match for the station name, \n",
    "                     or None if no match is found above the minimum score threshold.\n",
    "    \"\"\"\n",
    "    if station_name is None:\n",
    "        return None\n",
    "\n",
    "    best_match = None\n",
    "    best_score = 0\n",
    "\n",
    "    for name in station_to_borough.keys():\n",
    "        score = fuzz.token_sort_ratio(station_name, name)\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_match = name\n",
    "\n",
    "    return station_to_borough[best_match] if best_match and best_score >= min_score else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0768cd70",
   "metadata": {},
   "outputs": [],
   "source": [
    "station_to_borough = {row['name']: row['borough'] for _, row in bike_locs.iterrows()}\n",
    "empty_boroughs = merged_df[(merged_df['StartBorough'].isna()) | (merged_df['EndBorough'].isna())]\n",
    "\n",
    "\n",
    "# function to perform fuzzy matching in parallel\n",
    "def parallel_fuzzy_match(column):\n",
    "    return column.apply(fuzzy_match)\n",
    "\n",
    "# split the DataFrame into chunks for parallel processing\n",
    "num_parallel_tasks = 6\n",
    "chunk_size = len(empty_boroughs) // num_parallel_tasks  \n",
    "chunks = [empty_boroughs[i:i+chunk_size] for i in range(0, len(empty_boroughs), chunk_size)]\n",
    "\n",
    "# update the StartBorough column, process chunks in parallel\n",
    "with futures.ThreadPoolExecutor() as executor: \n",
    "    results = list(executor.map(parallel_fuzzy_match, [chunk['StartStation Name'] for chunk in chunks]))\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    chunk = chunks[i]\n",
    "    chunk.loc[:, 'StartBorough'] = result\n",
    "\n",
    "# update the EndBorough column, process chunks in parallel\n",
    "with futures.ThreadPoolExecutor() as executor:  # Use ThreadPoolExecutor for threads or ProcessPoolExecutor for processes\n",
    "    results = list(executor.map(parallel_fuzzy_match, [chunk['EndStation Name'] for chunk in chunks]))\n",
    "\n",
    "for i, result in enumerate(results):\n",
    "    chunk = chunks[i]\n",
    "    chunk.loc[:, 'EndBorough'] = result\n",
    "\n",
    "# replace the rows with missing borough data in the original dataframe with the processed rows\n",
    "updated_empty_boroughs = pd.concat(chunks)\n",
    "merged_df.update(updated_empty_boroughs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "52914937",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0                 0\n",
      "Rental Id                  0\n",
      "End Date                   0\n",
      "EndStation Name            0\n",
      "Start Date                 0\n",
      "StartStation Name          0\n",
      "StartBorough         1082424\n",
      "EndBorough           1145911\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# reduced number of missing values by 3.\n",
    "print(merged_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fda3b24",
   "metadata": {},
   "source": [
    "### Adding boroughs to former stations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4801bf1e",
   "metadata": {},
   "source": [
    "After investigating the missing borough data in the current samplers, it was discovered that these samplers correspond to former stations that are no longer in use and are not listed in the provided BikePoints file, where the station names are given in the format \"street name, region\" (e.g., \"London Fields, Hackney Central\").\n",
    "\n",
    "To address this a dictionary with the region information as key and the borough with the maximum counts as value is created. This is then used to map the missing boroughs in the merged_df DataFrame based on the extracted location information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "fa538f54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a 'location' column to bike_locs\n",
    "bike_locs['location'] = bike_locs['name'].str.split(',').str[1].str.strip()\n",
    "\n",
    "# group by 'location' and get the borough with the maximum counts\n",
    "location_borough = bike_locs.groupby('location')['borough'].agg(lambda x: x.value_counts().index[0])\n",
    "\n",
    "# convert the Series to a dictionary\n",
    "location_borough_dict = location_borough.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "a8ab178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_borough_from_dict(name):\n",
    "    \"\"\"\n",
    "    Retrieves the borough from the 'location_borough_dict' dictionary based on the given station name.\n",
    "    \n",
    "    Args:\n",
    "        name (str): Station name in the format 'street name, region'.\n",
    "    \n",
    "    Returns:\n",
    "        str or None: The corresponding borough based on the region, or None if the borough is unavailable.\n",
    "    \"\"\"\n",
    "    parts = name.split(',')\n",
    "    if len(parts) > 1:\n",
    "        return location_borough_dict.get(parts[1].strip(), None)\n",
    "    else:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "7561290a",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df.loc[merged_df['StartBorough'].isna(), 'StartBorough'] = merged_df.loc[merged_df['StartBorough'].isna(), 'StartStation Name'].apply(get_borough_from_dict)\n",
    "merged_df.loc[merged_df['EndBorough'].isna(), 'EndBorough'] = merged_df.loc[merged_df['EndBorough'].isna(), 'EndStation Name'].apply(get_borough_from_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "641d580f",
   "metadata": {},
   "source": [
    "### Map manually and drop test/workshop stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "a4418a0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Abingdon Green, Great College Street\n",
      "Allington street, Off Victoria Street, Westminster\n",
      "Columbia Road, Weavers\n",
      "Contact Centre, Southbury House\n",
      "Electrical Workshop PS\n",
      "Hansard Mews, Shepherds Bush\n",
      "Import Dock\n",
      "LSP1\n",
      "LSP2\n",
      "Mechanical Workshop Clapham\n",
      "Mechanical Workshop Penton\n",
      "Monier Road\n",
      "Monier Road, Newham\n",
      "One London\n",
      "Oval Way, Lambeth\n",
      "PENTON STREET COMMS TEST TERMINAL _ CONTACT MATT McNULTY\n",
      "Pop Up Dock 1\n",
      "Pop Up Dock 2\n",
      "Victoria and Albert Museum, Cromwell Road\n",
      "Worship Street, Hackney\n",
      "York Way, Camden\n"
     ]
    }
   ],
   "source": [
    "unique_empty_start_boroughs = merged_df.loc[merged_df['StartBorough'].isna(), 'StartStation Name'].unique()\n",
    "unique_empty_end_boroughs = merged_df.loc[merged_df['EndBorough'].isna(), 'EndStation Name'].unique()\n",
    "\n",
    "for name in unique_empty_boroughs:\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "21a4d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fill values manually\n",
    "\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Hansard Mews, Shepherds Bush', 'StartBorough'] = 'Hammersmith and Fulham'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Columbia Road, Weavers', 'StartBorough'] = 'Tower Hamlets'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Abingdon Green, Great College Street', 'StartBorough'] = 'Westminster'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Oval Way, Lambeth', 'StartBorough'] = 'Lambeth'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Contact Centre, Southbury House', 'StartBorough'] = 'Enfield'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Monier Road', 'StartBorough'] = 'Newham'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Victoria and Albert Museum, Cromwell Road', 'StartBorough'] = 'Kensington and Chelsea'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Monier Road, Newham', 'StartBorough'] = 'Newham'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Allington street, Off Victoria Street, Westminster', 'StartBorough'] = 'Westminster'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Worship Street, Hackney', 'StartBorough'] = 'Hackney'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'York Way, Camden', 'StartBorough'] = 'Camden'\n",
    "merged_df.loc[merged_df['StartStation Name'] == 'Monier Road', 'StartBorough'] = 'Hackney'\n",
    "\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Hansard Mews, Shepherds Bush', 'EndBorough'] = 'Hammersmith and Fulham'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Columbia Road, Weavers', 'EndBorough'] = 'Tower Hamlets'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Abingdon Green, Great College Street', 'EndBorough'] = 'Westminster'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Oval Way, Lambeth', 'EndBorough'] = 'Lambeth'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Contact Centre, Southbury House', 'EndBorough'] = 'Enfield'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Monier Road', 'EndBorough'] = 'Newham'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Victoria and Albert Museum, Cromwell Road', 'EndBorough'] = 'Kensington and Chelsea'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Monier Road, Newham', 'EndBorough'] = 'Newham'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Allington street, Off Victoria Street, Westminster', 'EndBorough'] = 'Westminster'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Worship Street, Hackney', 'EndBorough'] = 'Hackney'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'York Way, Camden', 'EndBorough'] = 'Camden'\n",
    "merged_df.loc[merged_df['EndStation Name'] == 'Monier Road', 'EndBorough'] = 'Hackney'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "2c84fdb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0           0\n",
      "Rental Id            0\n",
      "End Date             0\n",
      "EndStation Name      0\n",
      "Start Date           0\n",
      "StartStation Name    0\n",
      "StartBorough         0\n",
      "EndBorough           0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# drop test and workshop stations\n",
    "\n",
    "merged_df = merged_df.dropna(subset=['StartBorough', 'EndBorough'])\n",
    "print(merged_df.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71655ae5",
   "metadata": {},
   "source": [
    "# 6. Save cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "5c45f82c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all data (2015 - 20122)\n",
    "merged_df.to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_cleaned_with_boroughs_no_nans.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "65c93ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# by year\n",
    "\n",
    "groups = merged_df.groupby(pd.Grouper(key='Start Date', freq='Y'))\n",
    "yearly_dfs = {}\n",
    "for year, group in groups:\n",
    "    yearly_dfs[year.year] = group.reset_index(drop=True)\n",
    "    \n",
    "yearly_dfs[2015].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2015_cleaned.csv')\n",
    "yearly_dfs[2016].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2016_cleaned.csv')\n",
    "yearly_dfs[2017].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2017_cleaned.csv')\n",
    "yearly_dfs[2018].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2018_cleaned.csv')\n",
    "yearly_dfs[2019].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2019_cleaned.csv')\n",
    "yearly_dfs[2020].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2020_cleaned.csv')\n",
    "yearly_dfs[2021].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2021_cleaned.csv')\n",
    "yearly_dfs[2022].to_csv('/Users/tabea/Documents/UrbanMobility/data/journey_data_2022_cleaned.csv')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "history_visible": true,
   "machine_shape": "hm",
   "provenance": []
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
